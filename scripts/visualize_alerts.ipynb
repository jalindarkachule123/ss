{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1aeb627",
   "metadata": {},
   "source": [
    "# SOC Alert Analysis and Visualization\n",
    "\n",
    "This notebook provides visualizations and analysis of security alerts from our SOC environment, including:\n",
    "1. Alert trends over time\n",
    "2. Severity distribution\n",
    "3. XAI explanations\n",
    "4. Geographic distribution\n",
    "5. Attack type patterns\n",
    "\n",
    "## Setup and Requirements\n",
    "First, let's import the required libraries and set up our connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eb96ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# Load configuration\n",
    "with open('../config.json') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Set up authentication for Sentinel\n",
    "workspace_id = config['sentinel']['workspaceId']\n",
    "workspace_key = config['sentinel']['primaryKey']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e7e340",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "Let's fetch alerts from both Wazuh (via Elasticsearch) and Sentinel for the past 24 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cd01ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_elasticsearch_alerts(hours=24):\n",
    "    \"\"\"Fetch alerts from Elasticsearch.\"\"\"\n",
    "    es_host = config['elasticsearch']['host']\n",
    "    es_port = config['elasticsearch']['port']\n",
    "    es_user = config['elasticsearch']['username']\n",
    "    es_pass = config['elasticsearch']['password']\n",
    "    \n",
    "    # Calculate time range\n",
    "    end_time = datetime.utcnow()\n",
    "    start_time = end_time - timedelta(hours=hours)\n",
    "    \n",
    "    # Elasticsearch query\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"range\": {\n",
    "                \"@timestamp\": {\n",
    "                    \"gte\": start_time.isoformat(),\n",
    "                    \"lte\": end_time.isoformat()\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"sort\": [{\"@timestamp\": \"desc\"}],\n",
    "        \"size\": 10000\n",
    "    }\n",
    "    \n",
    "    # Make request to Elasticsearch\n",
    "    url = f\"http://{es_host}:{es_port}/wazuh-alerts-*/_search\"\n",
    "    response = requests.get(url, json=query, auth=(es_user, es_pass))\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        hits = response.json()['hits']['hits']\n",
    "        alerts = [hit['_source'] for hit in hits]\n",
    "        return pd.DataFrame(alerts)\n",
    "    else:\n",
    "        print(f\"Error fetching alerts: {response.status_code}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Fetch alerts\n",
    "df_alerts = fetch_elasticsearch_alerts()\n",
    "print(f\"Retrieved {len(df_alerts)} alerts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b73053",
   "metadata": {},
   "source": [
    "## Alert Trends\n",
    "Let's visualize the alert trends over time, including:\n",
    "1. Alert frequency by hour\n",
    "2. Severity distribution\n",
    "3. Top alert rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e129702d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to datetime and set as index\n",
    "df_alerts['@timestamp'] = pd.to_datetime(df_alerts['@timestamp'])\n",
    "df_alerts.set_index('@timestamp', inplace=True)\n",
    "\n",
    "# Alert frequency by hour\n",
    "hourly_alerts = df_alerts.resample('H').size()\n",
    "\n",
    "# Create time series plot\n",
    "fig = px.line(hourly_alerts, \n",
    "              title='Alert Frequency Over Time',\n",
    "              labels={'index': 'Time', 'value': 'Number of Alerts'})\n",
    "fig.show()\n",
    "\n",
    "# Severity distribution\n",
    "fig_severity = px.histogram(df_alerts, \n",
    "                          x='rule.level',\n",
    "                          title='Alert Severity Distribution',\n",
    "                          labels={'rule.level': 'Severity Level', 'count': 'Number of Alerts'},\n",
    "                          nbins=20)\n",
    "fig_severity.show()\n",
    "\n",
    "# Top alert rules\n",
    "top_rules = df_alerts['rule.description'].value_counts().head(10)\n",
    "fig_rules = px.bar(top_rules,\n",
    "                  title='Top 10 Alert Rules',\n",
    "                  labels={'index': 'Rule Description', 'value': 'Count'})\n",
    "fig_rules.update_layout(xaxis_tickangle=45)\n",
    "fig_rules.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b6bb6c",
   "metadata": {},
   "source": [
    "## XAI Analysis\n",
    "Now let's examine the XAI explanations for high-severity alerts to understand the model's decision-making process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79337c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter high-severity alerts (level > 10)\n",
    "high_severity = df_alerts[df_alerts['rule.level'] > 10].copy()\n",
    "\n",
    "# Extract XAI features and explanations\n",
    "if 'xai_analysis' in high_severity.columns:\n",
    "    # Create feature importance plot\n",
    "    feature_importance = pd.DataFrame([\n",
    "        alert['xai_analysis'].get('feature_importance', [])\n",
    "        for alert in high_severity['xai_analysis']\n",
    "    ])\n",
    "    \n",
    "    # Average feature importance across all high-severity alerts\n",
    "    avg_importance = feature_importance.mean()\n",
    "    \n",
    "    # Plot feature importance\n",
    "    fig_xai = px.bar(avg_importance,\n",
    "                     title='Average Feature Importance for High-Severity Alerts',\n",
    "                     labels={'index': 'Feature', 'value': 'Importance Score'})\n",
    "    fig_xai.update_layout(xaxis_tickangle=45)\n",
    "    fig_xai.show()\n",
    "    \n",
    "    # Show sample explanations\n",
    "    print(\"\\nSample XAI Explanations for High-Severity Alerts:\")\n",
    "    for idx, row in high_severity.head().iterrows():\n",
    "        print(f\"\\nAlert at {idx}:\")\n",
    "        print(f\"Rule: {row['rule.description']}\")\n",
    "        print(f\"XAI Explanation: {row['xai_analysis'].get('explanation_text', 'No explanation available')}\")\n",
    "else:\n",
    "    print(\"No XAI analysis found in the alerts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2f0034",
   "metadata": {},
   "source": [
    "## Geographic Distribution\n",
    "Let's visualize the geographic distribution of alerts using source IP addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd61677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract source IPs and their frequencies\n",
    "if 'source_ip' in df_alerts.columns:\n",
    "    ip_counts = df_alerts['source_ip'].value_counts()\n",
    "    \n",
    "    # Create choropleth map\n",
    "    # Note: This assumes we have a way to map IPs to countries\n",
    "    # You might want to use a geolocation service or database\n",
    "    \n",
    "    fig_geo = go.Figure(data=go.Choropleth(\n",
    "        # Add your geolocation data here\n",
    "        locationmode='country names',\n",
    "        colorbar_title='Number of Alerts'\n",
    "    ))\n",
    "    \n",
    "    fig_geo.update_layout(\n",
    "        title='Geographic Distribution of Alert Sources',\n",
    "        geo=dict(showframe=False, showcoastlines=True, projection_type='equirectangular'),\n",
    "    )\n",
    "    fig_geo.show()\n",
    "    \n",
    "    # Show top source countries\n",
    "    print(\"\\nTop Source Countries:\")\n",
    "    # Add your country resolution logic here\n",
    "else:\n",
    "    print(\"No source IP information found in alerts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63e260c",
   "metadata": {},
   "source": [
    "## Summary and Recommendations\n",
    "Based on the analysis above, let's generate some security recommendations and insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ef8493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary statistics\n",
    "summary = {\n",
    "    'total_alerts': len(df_alerts),\n",
    "    'high_severity': len(df_alerts[df_alerts['rule.level'] > 10]),\n",
    "    'unique_sources': df_alerts['source_ip'].nunique() if 'source_ip' in df_alerts.columns else 0,\n",
    "    'top_rules': df_alerts['rule.description'].value_counts().head(5).to_dict()\n",
    "}\n",
    "\n",
    "print(\"Alert Analysis Summary:\")\n",
    "print(f\"Total Alerts: {summary['total_alerts']}\")\n",
    "print(f\"High Severity Alerts: {summary['high_severity']}\")\n",
    "print(f\"Unique Source IPs: {summary['unique_sources']}\")\n",
    "print(\"\\nTop Alert Rules:\")\n",
    "for rule, count in summary['top_rules'].items():\n",
    "    print(f\"- {rule}: {count}\")\n",
    "\n",
    "# Generate recommendations based on the analysis\n",
    "print(\"\\nRecommendations:\")\n",
    "if summary['high_severity'] > 0:\n",
    "    print(\"- High priority: Review and address high-severity alerts\")\n",
    "    print(\"- Consider adjusting detection thresholds for frequently triggering rules\")\n",
    "    print(\"- Investigate patterns in source IPs for potential targeted attacks\")\n",
    "    \n",
    "if 'xai_analysis' in df_alerts.columns:\n",
    "    print(\"- Review XAI explanations for insight into alert patterns\")\n",
    "    print(\"- Use feature importance data to tune detection models\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
